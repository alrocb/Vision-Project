
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wsi_directories(data_dir):\n",
    "    wsi_directories = [os.path.join(data_dir, folder) for folder in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, folder))]\n",
    "    return wsi_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_patch_paths_from_folder_list(folder_list):\n",
    "    patch_paths = []\n",
    "    for folder_path in folder_list:\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith('.png'):\n",
    "                patch_paths.append(os.path.join(folder_path, file))\n",
    "    return patch_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, patch_paths, transform=None):\n",
    "        self.patch_paths = patch_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patch_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        patch_path = self.patch_paths[idx]\n",
    "        image = Image.open(patch_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_dir = \"/fhome/mapsiv/QuironHelico/CroppedPatches/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_patch_paths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5b96b702f6de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpatches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_patch_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'get_patch_paths' is not defined"
     ]
    }
   ],
   "source": [
    "patches=get_patch_paths(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders= get_wsi_directories(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_img=len(folders)\n",
    "train_imgs=int(num_img*0.7)\n",
    "test_imgs= int(num_img-train_imgs)\n",
    "\n",
    "print(\"Tenemos\",num_img,\"imagenes.\")\n",
    "print(\"El train constará de\",train_imgs,\"imagenes.\")\n",
    "print(\"El test set constará de\",test_imgs,\"imagenes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images=folders[0:171]\n",
    "print(len(train_images))\n",
    "\n",
    "test_images=folders[171:]\n",
    "print(len(test_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patches=get_patch_paths_from_folder_list(train_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patches[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(train_patches)/32)/60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_patches=get_patch_paths_from_folder_list(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((253, 253)), transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_patches, transform)\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_patches, transform)\n",
    "batch_size = 1\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    for image in batch:\n",
    "      img=image\n",
    "      print(batch.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Desactiva los ejes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Bloque 1: Conv1 -> BatchNorm1 -> LeakyReLU1\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()  # Puedes usar Sigmoid para la reconstrucción si tus imágenes están en el rango [0, 1]\n",
    "        )\n",
    "\n",
    "        # Bloque 2: Conv2 -> BatchNorm2 -> LeakyReLU2\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Bloque 3: Conv3 -> BatchNorm3 -> LeakyReLU3\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Codificación\n",
    "        x1 = self.encoder1(x)\n",
    "        x2 = self.encoder2(x1)\n",
    "        x3 = self.encoder3(x2)\n",
    "        \n",
    "        # Decodificación\n",
    "        y2 = self.decoder3(x3)\n",
    "        y1 = self.decoder2(y2)\n",
    "        y = self.decoder1(y1)\n",
    "\n",
    "        return y\n",
    "\n",
    "# Crea una instancia del modelo\n",
    "autoencoder = Autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    for image in batch:\n",
    "      img=image\n",
    "      print(batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Define el optimizador (por ejemplo, SGD)\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "# Número de épocas\n",
    "num_epochs = 1\n",
    "\n",
    "# Entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        # Obtén los datos de entrada\n",
    "        inputs = batch\n",
    "\n",
    "        # Pasa los datos al dispositivo (por ejemplo, GPU)\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        # Reinicia los gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Propagación hacia adelante\n",
    "        outputs = autoencoder(inputs)\n",
    "\n",
    "        # Calcula la pérdida de reconstrucción\n",
    "        loss = criterion(outputs, inputs)\n",
    "\n",
    "        # Retropropagación y optimización\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Estadísticas de pérdida\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Imprime la pérdida promedio en cada época\n",
    "    print(f'Época {epoch + 1}, Pérdida: {running_loss / len(dataloader)}')\n",
    "\n",
    "print('Entrenamiento completado')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtEnvPython",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
